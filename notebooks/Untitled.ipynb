{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e7b3f1-c98b-4220-ba92-64ac47f6f367",
   "metadata": {},
   "source": [
    "# LangChain + OpaneAI RAG Proof of Concept\n",
    "## Overview\n",
    "* Collates provided sources into local vector store\n",
    "* Persists store for future recovery\n",
    "* Identifies relevant chunks and constructs prompt to send to OpenAI\n",
    "### Dependencies\n",
    "* User account for OpenAI and LangSmith\n",
    "* API keys from OpenAI and LangSmith\n",
    "* Main python dependencies:\n",
    "  * LangChain: langchain-openai langchain-core langchain-text-splitters langchain-community langgraph langchain[openai]\n",
    "  * GDrive: oogle-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-google-community\n",
    "  * Web: bs4  unstructured selenium\n",
    "* Add the following to `~/.bash_profile` or `~/.zprofile` if `~/.zshrc`\n",
    "```\n",
    "export LANGSMITH_TRACING=true\n",
    "export LANGSMITH_ENDPOINT=\"https://{langsmith_region}.api.smith.langchain.com\"\n",
    "export LANGSMITH_API_KEY=\"{your_langsmith_api_key}\"\n",
    "export OPENAI_API_KEY=\"{your_openai_api_key}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c485118-e803-498c-b162-c8e6b9eb1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith: \")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"rag-ingestion-pipeline/1.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c362d1-563e-43fd-bba5-80f74d615459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 3 + 3 is 6.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 18, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZQLQiUMDQDFgR3LoaEiSlwStHPQm', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ae212afe-bc4e-4a9f-9c5e-81beec81bbb4-0', usage_metadata={'input_tokens': 18, 'output_tokens': 11, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test basic setup is working\n",
    "# Should see an appropriate response, and also updated metrics in LangSmith dashboard\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world! what's 3+3?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc2cd4e-3fb4-4e7b-bd06-3e67e6f64bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01016364898532629, 0.02342759631574154, -0.04225384443998337, -0.0015080638695508242, -0.023511\n"
     ]
    }
   ],
   "source": [
    "# Set up an embeding scheme to translate and search documents \n",
    "# This will also be used if loading vector store from disk using FAISS \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "sample_text = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "sample_embedded_vector = embedding_model.embed_documents([sample_text])\n",
    "print(str(sample_embedded_vector)[:100])  # Show the first 100 characters of the vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7f2c34-ee2f-4f22-8328-741f3d80c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded from '/Users/jsouthin/Projects/rag/faiss_index'.\n"
     ]
    }
   ],
   "source": [
    "# Utility functions to support loading, saving updating and initialisint vector stores\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.vectorstores import FAISS\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "\n",
    "# In-memory store\n",
    "_vector_store = None\n",
    "\n",
    "\n",
    "def init_vector_store(docs, embeddings):\n",
    "    global _vector_store\n",
    "    _vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "def save_vector_store(path: str, force: bool = False):\n",
    "    if Path(path).exists() and force == False:\n",
    "        raise FileExistsError(f\"Path '{path}' already exists. Use a different name or remove the existing store.\")\n",
    "    if _vector_store is None:\n",
    "        raise RuntimeError(\"No vector store in memory to save.\")\n",
    "    _vector_store.save_local(path)\n",
    "    print(f\"New vector store saved to '{path}'.\")\n",
    "\n",
    "\n",
    "def load_vector_store(path: str, embedding_model, force: bool = False, **kwargs):\n",
    "    global _vector_store\n",
    "    if _vector_store is not None and not force:\n",
    "        raise RuntimeError(\"A vector store is already loaded in memory. Use force=True to override.\")\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Vector store path '{path}' does not exist.\")\n",
    "    if \"allow_dangerous_deserialization\" not in kwargs:\n",
    "        raise ValueError(\"The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).\")\n",
    "    _vector_store = FAISS.load_local(path, embedding_model, allow_dangerous_deserialization=kwargs[\"allow_dangerous_deserialization\"])\n",
    "    print(f\"Vector store loaded from '{path}'.\")\n",
    "\n",
    "\n",
    "def update_vector_store(new_docs: list[Document], save_path: str, force: bool = False):\n",
    "    global _vector_store\n",
    "    if _vector_store is None:\n",
    "        raise RuntimeError(\"No vector store in memory. Load or initialize it first.\")        \n",
    "    _vector_store.add_documents(new_docs)\n",
    "    print(f\"Vector store updated with {len(new_docs)} new documents.\")\n",
    "    save_vector_store(save_path, force)\n",
    "\n",
    "\n",
    "def get_vector_store():\n",
    "    if _vector_store is None:\n",
    "        raise RuntimeError(\"No vector store in memory.\")\n",
    "    return _vector_store\n",
    "\n",
    "vector_store_path = \"/Users/jsouthin/Projects/rag/faiss_index\"\n",
    "load_vector_store(vector_store_path, embedding_model, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2febb5-1504-4cb1-9bb9-a99863a0a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current vector stor: <langchain_community.vectorstores.faiss.FAISS object at 0x12159b6d0>\n",
      "Vector store contains 1 searchable documents.\n"
     ]
    }
   ],
   "source": [
    "# verify it's working.  Expect at least 1 searchable document\n",
    "def test_vector_store():\n",
    "    if _vector_store is None:\n",
    "        raise RuntimeError(\"No vector store loaded in memory.\")\n",
    "    \n",
    "    try:\n",
    "        # Try retrieving a dummy query\n",
    "        print(f'Current vector stor: {get_vector_store()}')\n",
    "        results = _vector_store.similarity_search(\"test\", k=1)\n",
    "        if results:\n",
    "            print(f\"Vector store contains {len(results)} searchable documents.\")\n",
    "        else:\n",
    "            print(\"Vector store loaded but no searchable results found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing vector store: {e}\")\n",
    "test_vector_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9689c3b3-3387-4931-8bb5-713f2f894397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents/chunks in vector store: 225\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of documents/chunks in vector store: {_vector_store.index.ntotal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439cd874-5f80-41f6-88f1-a9e841169474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1bb6c4-b6d3-4675-8423-cc01f5015171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pypdf._reader:Ignoring wrong pointing object 9 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 15 0 (offset 0)\n",
      "/Users/jsouthin/Projects/rag/document_loaders.py:140: LangChainDeprecationWarning: The class `GoogleDriveLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import GoogleDriveLoader``.\n",
      "  loader = GoogleDriveLoader(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from document_loaders import load_web_document, load_pdf_document, load_google_drive_document, load_notion_document, load_dynamic_web_document\n",
    "\n",
    "project_id = \"my_first_project\"\n",
    "\n",
    "all_docs = load_dynamic_web_document([\"https://www.greatyellow.earth\",\"https://www.greatyellow.earth/about\"],project_id=project_id) \\\n",
    "    + load_pdf_document([\"/Users/jsouthin/Documents/Joe Southin - CV 2025 (A4).pdf\",\"/Users/jsouthin/Downloads/joe southin cv 2016.pdf\"],project_id=project_id) \\\n",
    "    + load_google_drive_document([\"1xCW-ZiquUxwBLpMTn9kL6WrvPlfeBYpDuicQhiIR81w\"],\"/Users/jsouthin/Downloads/lively-tensor-432422-c0-407d22e805d2.json\",project_id=project_id) \\\n",
    "    #+ load_notion_documents(token, [...])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcde88a-3c35-4d0c-9e69-fb88bd4d98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = _vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "#prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "'''\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    project_id: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = _vector_store.similarity_search(state[\"question\"])\n",
    "    filtered_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"project_id\") == state[\"project_id\"]]\n",
    "    return {\"context\": filtered_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050b2220-a2d7-4406-8374-b9d10a2a29d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vector store saved to '/Users/jsouthin/Projects/rag/faiss_index'.\n"
     ]
    }
   ],
   "source": [
    "save_vector_store(vector_store_path,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff95033-a5c9-4db0-b998-5560db55a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current vector stor: <langchain_community.vectorstores.faiss.FAISS object at 0x12159b6d0>\n",
      "Vector store contains 1 searchable documents.\n"
     ]
    }
   ],
   "source": [
    "test_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b16e8c18-1057-423c-8aa5-69a7a4be9ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A type 2 model is known as the Customer id OHE model, while a type 3 model is referred to as the Campaign id OHE model. The main difference lies in the features used during training - for the type 2 model, the pca_id is one hot encoded, while for the type 3 model, both campaign id and client id columns are one hot encoded. In addition, the type 3 model can only generate inference or simulations for campaigns present in the previous week's training set, while type 2 can handle those not present in the training set.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"explain the differences between a type 1, 2, and 3 model\",\"project_id\":project_id})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f5181d-82f9-4405-b9b3-5abd215710d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name Great Yellow is inspired by the Great Yellow bumblebee, a species that symbolizes the beauty and fragility of nature. It highlights the importance of protecting biodiversity and nurturing natural systems. Great Yellow aims to demonstrate that thriving ecosystems are economically valuable.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"why the name Great Yellow?\",\"project_id\":project_id})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21256622-7f4d-4083-9bdf-ec423bb7c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Southin's biggest accomplishment is developing an ML-powered budget management solution that optimized spend allocation, increased customer retention, and now manages $78M+ in annual ad-spend.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is joe southin's biggest accomplishment?\",\"project_id\":project_id})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3b149-1d35-4c17-952e-2bb6a80af0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
